---
title: "COD Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

#### Article ID: gIjYG
#### Pilot: Minyoung Lee
#### Co-pilot: Tom Hardwicke
#### Start date: Mar 24 2017
#### End date: Nov 2 2017
#### Final verification: Tom Hardwicke
#### Date: Nov 9 2017

-------

#### Methods summary: 

Participants searched for a target in complex scenes containing social or non-social distractors. The search time and whether or not the distractor was fixated first after scene onset (first look) were measured in each search trial. The search time and first look in social and non-social trials were compared over three blocks. Learning in visual search waas operationalized as decreasing search time over blocks.

------

#### Target outcomes: 

> 3.1. Visual search

> Manual and eye-tracking measures provided converging evidence for social stimuli being more distracting than non-social stimuli, with the effects interacting with learning over successive blocks (see Table 1 for descriptives).

> 3.1.1. Manual search time (s)

> A repeated-measures ANOVA with two within-subject factors (distractor: social, non-social; block: one, two, three), revealed a main effect of block, F(1.47, 52.89) = 395.23, p = < 0.001, η2 = 0.92, driven by decreasing search time across blocks. There was no main effect of distractor on search time, F(1, 36) = 0.38, p > 0.250, η2 = 0.01, but there was a significant distractor-by-block interaction, F(1.94, 69.98) = 3.78, p = 0.029, η2 = 0.10.

------

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)

# prepare an empty report object, we will update this each time we run compareValues2()
reportObject <- data.frame("Article_ID" = NA, "valuesChecked" = 0, "eyeballs" = 0, "Total_df" = 0, "Total_p" = 0, "Total_mean" = 0, "Total_sd" = 0, "Total_se" = 0, "Total_ci" = 0, "Total_bf" = 0, "Total_t" = 0, "Total_F" = 0, "Total_es" = 0, "Total_median" = 0, "Total_irr" = 0, "Total_r" = 0, "Total_z" = 0, "Total_coeff" = 0, "Total_n" = 0, "Total_x2" = 0, "Total_other" = 0, "Insufficient_Information_Errors" = 0, "Decision_Errors" = 0, "Major_Numerical_Errors" = 0, "Minor_Numerical_Errors" = 0, "Major_df" = 0, "Major_p" = 0, "Major_mean" = 0, "Major_sd" = 0, "Major_se" = 0, "Major_ci" = 0, "Major_bf" = 0, "Major_t" = 0, "Major_F" = 0, "Major_es" = 0, "Major_median" = 0, "Major_irr" = 0, "Major_r" = 0, "Major_z" = 0, "Major_coeff" = 0, "Major_n" = 0, "Major_x2" = 0, "Major_other" = 0, "affectsConclusion" = NA, "error_typo" = 0, "error_specification" = 0, "error_analysis" = 0, "error_data" = 0, "error_unidentified" = 0, "Author_Assistance" = NA, "resolved_typo" = 0, "resolved_specification" = 0, "resolved_analysis" = 0, "resolved_data" = 0, "correctionSuggested" = NA, "correctionPublished" = NA)
```

## Step 1: Load packages

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CODreports) # custom report functions
library(ez) # for anova
library(afex) # for anova (with author assistance)
library(plotrix) # for function to calculate standard error
```

## Step 2: Load data

```{r}
d <- read.csv("data/data.csv")
```

## Step 3: Tidy data

```{r}
d.tidy <- d %>% select(Sbj,social,block,learn_ACC,learn_total_search,Total_first_look,AQ,SAS) %>%
  rename(sbj = Sbj,
         accurate = learn_ACC,
         rt = learn_total_search,
         first = Total_first_look,
         distractor = social) %>%
  mutate(distractor = recode_factor(distractor, "1" = "social", "0" = "nonsocial"),
         sbj = factor(sbj),
         block = factor(block))
```

## Step 4: Run analysis

### Descriptive statistics

We will first try to reproduce the descriptive statistics reported in Table 1:

> Manual and eye-tracking measures provided converging evidence for social stimuli being more distracting than non-social stimuli, with the effects interacting with learning over successive blocks (see Table 1 for descriptives).

![](images/table1.png)


```{r}
# accuracy
tbl <- d.tidy %>% group_by(sbj, distractor, block) %>%
  summarise(accuracy = mean(accurate)*100)  %>%
  group_by(distractor, block) %>%
  summarise(meanAccuracy = mean(accuracy, na.rm = T), se_accuracy = std.error(accuracy, na.rm = T))

# RT and First look in accurate trials
tbl2 <- d.tidy %>% group_by(sbj, distractor, block) %>%
  filter(accurate==1) %>%
  summarise(RT = mean(rt), First = sum(first,na.rm=TRUE)/sum(!is.nan(first))*100) %>%
  group_by(distractor,block, add=FALSE) %>%
  summarise(meanRT = mean(RT), se_RT = std.error(RT), meanFirst = mean(First), se_first = std.error(First))

# join tables and make same format as Table 1
table1 <- left_join(tbl2, tbl) %>% select(distractor, block, meanRT, se_RT, meanAccuracy, se_accuracy, meanFirst, se_first)
kable(table1, digits = 2)
```

Visual comparison of our table 1 and the published table 1: all the means are accurately reproduced but all of the standard errors do no match.

Explictly check the means:

```{r}
# rt means
soc_rt_m_1 <- table1 %>% filter(distractor == 'social', block == 1) %>% pull(meanRT)
soc_rt_m_2 <- table1 %>% filter(distractor == 'social', block == 2) %>% pull(meanRT)
soc_rt_m_3 <- table1 %>% filter(distractor == 'social', block == 3) %>% pull(meanRT)
nsoc_rt_m_1 <- table1 %>% filter(distractor == 'nonsocial', block == 1) %>% pull(meanRT)
nsoc_rt_m_2 <- table1 %>% filter(distractor == 'nonsocial', block == 2) %>% pull(meanRT)
nsoc_rt_m_3 <- table1 %>% filter(distractor == 'nonsocial', block == 3) %>% pull(meanRT)

reportobject <- compareValues2(reportedValue = "4.85", obtainedValue = soc_rt_m_1, valueType = 'mean')
reportobject <- compareValues2(reportedValue = "3.32", obtainedValue = soc_rt_m_2, valueType = 'mean')
reportobject <- compareValues2(reportedValue = "2.60", obtainedValue = soc_rt_m_3, valueType = 'mean')
reportobject <- compareValues2(reportedValue = "5.00", obtainedValue = nsoc_rt_m_1, valueType = 'mean')
reportobject <- compareValues2(reportedValue = "3.45", obtainedValue = nsoc_rt_m_2, valueType = 'mean')
reportobject <- compareValues2(reportedValue = "2.53", obtainedValue = nsoc_rt_m_3, valueType = 'mean')

# accuracy means
soc_acc_m_1 <- table1 %>% filter(distractor == 'social', block == 1) %>% pull(meanAccuracy)
soc_acc_m_2 <- table1 %>% filter(distractor == 'social', block == 2) %>% pull(meanAccuracy)
soc_acc_m_3 <- table1 %>% filter(distractor == 'social', block == 3) %>% pull(meanAccuracy)
nsoc_acc_m_1 <- table1 %>% filter(distractor == 'nonsocial', block == 1) %>% pull(meanAccuracy)
nsoc_acc_m_2 <- table1 %>% filter(distractor == 'nonsocial', block == 2) %>% pull(meanAccuracy)
nsoc_acc_m_3 <- table1 %>% filter(distractor == 'nonsocial', block == 3) %>% pull(meanAccuracy)

reportobject <- compareValues2(reportedValue = "97.09", obtainedValue = soc_acc_m_1, valueType = 'mean')
reportobject <- compareValues2(reportedValue = "97.97", obtainedValue = soc_acc_m_2, valueType = 'mean')
reportobject <- compareValues2(reportedValue = "98.31", obtainedValue = soc_acc_m_3, valueType = 'mean')
reportobject <- compareValues2(reportedValue = "95.95", obtainedValue = nsoc_acc_m_1, valueType = 'mean')
reportobject <- compareValues2(reportedValue = "98.04", obtainedValue = nsoc_acc_m_2, valueType = 'mean')
reportobject <- compareValues2(reportedValue = "98.12", obtainedValue = nsoc_acc_m_3, valueType = 'mean')

# first look means
soc_look_m_1 <- table1 %>% filter(distractor == 'social', block == 1) %>% pull(meanFirst)
soc_look_m_2 <- table1 %>% filter(distractor == 'social', block == 2) %>% pull(meanFirst)
soc_look_m_3 <- table1 %>% filter(distractor == 'social', block == 3) %>% pull(meanFirst)
nsoc_look_m_1 <- table1 %>% filter(distractor == 'nonsocial', block == 1) %>% pull(meanFirst)
nsoc_look_m_2 <- table1 %>% filter(distractor == 'nonsocial', block == 2) %>% pull(meanFirst)
nsoc_look_m_3 <- table1 %>% filter(distractor == 'nonsocial', block == 3) %>% pull(meanFirst)

reportobject <- compareValues2(reportedValue = "26.49", obtainedValue = soc_look_m_1, valueType = 'mean')
reportobject <- compareValues2(reportedValue = "21.51", obtainedValue = soc_look_m_2, valueType = 'mean')
reportobject <- compareValues2(reportedValue = "18.60", obtainedValue = soc_look_m_3, valueType = 'mean')
reportobject <- compareValues2(reportedValue = "16.97", obtainedValue = nsoc_look_m_1, valueType = 'mean')
reportobject <- compareValues2(reportedValue = "15.52", obtainedValue = nsoc_look_m_2, valueType = 'mean')
reportobject <- compareValues2(reportedValue = "14.11", obtainedValue = nsoc_look_m_3, valueType = 'mean')
```

Now explictly check the SEs...

NOTE: this was our first attempt before author assistance. 

```{r}
# Check discrepancies in standard errors with compare values function:

# RT Standard Errors
# compareValues(0.11, 0.17)
# compareValues(0.06, 0.11)
# compareValues(0.06, 0.08)
# compareValues(0.11, 0.17)
# compareValues(0.07, 0.14)
# compareValues(0.07, 0.18)
# 
# # Accuracy Standard Errors
# compareValues(0.45, 0.77)
# compareValues(0.34, 0.59)
# compareValues(0.32, 0.43)
# compareValues(0.64, 0.78)
# compareValues(0.33, 0.51)
# compareValues(0.34, 0.65)
# 
# # First looks Standard Errors
# compareValues(1.81, 2.44)
# compareValues(1.39, 1.89)
# compareValues(1.12, 1.49)
# compareValues(1.27, 1.16)
# compareValues(1.23, 1.09)
# compareValues(1.17, 0.91)

# All are major numerical errors

```

NOTE: we asked the authors for assistance and they sent us more details. They said they used the following function to calculate within-subject standard errors:

```{r}
# define functions
# Gives count, mean, standard deviation, standard error of the mean, and confidence interval (default 95%).
#   data: a data frame.
#   measurevar: the name of a column that contains the variable to be summariezed
#   groupvars: a vector containing names of columns that contain grouping variables
#   na.rm: a boolean that indicates whether to ignore NA's
#   conf.interval: the percent range of the confidence interval (default is 95%)
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
                      conf.interval=.95, .drop=TRUE) {
    library(plyr)

    # New version of length which can handle NA's: if na.rm==T, don't count them
    length2 <- function (x, na.rm=FALSE) {
        if (na.rm) sum(!is.na(x))
        else       length(x)
    }

    # This does the summary. For each group's data frame, return a vector with
    # N, mean, and sd
    datac <- ddply(data, groupvars, .drop=.drop,
      .fun = function(xx, col) {
        c(N    = length2(xx[[col]], na.rm=na.rm),
          mean = mean   (xx[[col]], na.rm=na.rm),
          sd   = sd     (xx[[col]], na.rm=na.rm)
        )
      },
      measurevar
    )

    # Rename the "mean" column    
    datac <- rename(datac, c("mean" = measurevar))

    datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean

    # Confidence interval multiplier for standard error
    # Calculate t-statistic for confidence interval: 
    # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
    ciMult <- qt(conf.interval/2 + .5, datac$N-1)
    datac$ci <- datac$se * ciMult

    return(datac)
}

# Norms the data within specified groups in a data frame; it normalizes each
# subject (identified by idvar) so that they have the same mean, within each group
# specified by betweenvars.
#   data: a data frame.
#   idvar: the name of a column that identifies each subject (or matched subjects)
#   measurevar: the name of a column that contains the variable to be summariezed
#   betweenvars: a vector containing names of columns that are between-subjects variables
#   na.rm: a boolean that indicates whether to ignore NA's
normDataWithin <- function(data=NULL, idvar, measurevar, betweenvars=NULL,
                           na.rm=FALSE, .drop=TRUE) {
    library(plyr)

    # Measure var on left, idvar + between vars on right of formula.
    data.subjMean <- ddply(data, c(idvar, betweenvars), .drop=.drop,
     .fun = function(xx, col, na.rm) {
        c(subjMean = mean(xx[,col], na.rm=na.rm))
      },
      measurevar,
      na.rm
    )

    # Put the subject means with original data
    data <- merge(data, data.subjMean)

    # Get the normalized data in a new column
    measureNormedVar <- paste(measurevar, "_norm", sep="")
    data[,measureNormedVar] <- data[,measurevar] - data[,"subjMean"] +
                               mean(data[,measurevar], na.rm=na.rm)

    # Remove this subject mean column
    data$subjMean <- NULL

    return(data)
}

# Summarizes data, handling within-subjects variables by removing inter-subject variability.
# It will still work if there are no within-S variables.
# Gives count, un-normed mean, normed mean (with same between-group mean),
#   standard deviation, standard error of the mean, and confidence interval.
# If there are within-subject variables, calculate adjusted values using method from Morey (2008).
#   data: a data frame.
#   measurevar: the name of a column that contains the variable to be summariezed
#   betweenvars: a vector containing names of columns that are between-subjects variables
#   withinvars: a vector containing names of columns that are within-subjects variables
#   idvar: the name of a column that identifies each subject (or matched subjects)
#   na.rm: a boolean that indicates whether to ignore NA's
#   conf.interval: the percent range of the confidence interval (default is 95%)
summarySEwithin <- function(data=NULL, measurevar, betweenvars=NULL, withinvars=NULL,
                            idvar=NULL, na.rm=FALSE, conf.interval=.95, .drop=TRUE) {

  # Ensure that the betweenvars and withinvars are factors
  factorvars <- vapply(data[, c(betweenvars, withinvars), drop=FALSE],
    FUN=is.factor, FUN.VALUE=logical(1))

  if (!all(factorvars)) {
    nonfactorvars <- names(factorvars)[!factorvars]
    message("Automatically converting the following non-factors to factors: ",
            paste(nonfactorvars, collapse = ", "))
    data[nonfactorvars] <- lapply(data[nonfactorvars], factor)
  }

  # Get the means from the un-normed data
  datac <- summarySE(data, measurevar, groupvars=c(betweenvars, withinvars),
                     na.rm=na.rm, conf.interval=conf.interval, .drop=.drop)

  # Drop all the unused columns (these will be calculated with normed data)
  datac$sd <- NULL
  datac$se <- NULL
  datac$ci <- NULL

  # Norm each subject's data
  ndata <- normDataWithin(data, idvar, measurevar, betweenvars, na.rm, .drop=.drop)

  # This is the name of the new column
  measurevar_n <- paste(measurevar, "_norm", sep="")

  # Collapse the normed data - now we can treat between and within vars the same
  ndatac <- summarySE(ndata, measurevar_n, groupvars=c(betweenvars, withinvars),
                      na.rm=na.rm, conf.interval=conf.interval, .drop=.drop)

  # Apply correction from Morey (2008) to the standard error and confidence interval
  #  Get the product of the number of conditions of within-S variables
  nWithinGroups    <- prod(vapply(ndatac[,withinvars, drop=FALSE], FUN=nlevels,
                           FUN.VALUE=numeric(1)))
  correctionFactor <- sqrt( nWithinGroups / (nWithinGroups-1) )

  # Apply the correction factor
  ndatac$sd <- ndatac$sd * correctionFactor
  ndatac$se <- ndatac$se * correctionFactor
  ndatac$ci <- ndatac$ci * correctionFactor

  # Combine the un-normed means with the normed results
  merge(datac, ndatac)
}
```

Apply the function

Accuracy ses:

```{r}
acc_se <- d.tidy %>% 
  group_by(sbj, distractor, block) %>%
  dplyr::summarise(accuracy = mean(accurate)*100) %>% # aggregate first
  summarySEwithin(measurevar = 'accuracy', withinvars = c('distractor', 'block'), idvar = 'sbj', na.rm = T) %>%
  select(distractor, block, mean = accuracy, se)
```

Now get RT ses for accurate trials only:

```{r}
rt_se <- d.tidy %>% 
  group_by(sbj, distractor, block) %>%
  filter(accurate==1) %>%
  dplyr::summarise(RT = mean(rt), First = sum(first,na.rm=TRUE)/sum(!is.nan(first))*100) %>% # first aggregate
  summarySEwithin(measurevar = 'RT', withinvars = c('distractor', 'block'), idvar = 'sbj', na.rm = T) %>%
  select(distractor, block, mean = RT, se)
```

Now get first look ses for accurate trials only:

```{r}
look_se <- d.tidy %>% 
  group_by(sbj, distractor, block) %>%
  filter(accurate==1) %>%
  dplyr::summarise(First = mean(first, na.rm = T)*100) %>% # first aggregate
  summarySEwithin(measurevar = 'First', withinvars = c('distractor', 'block'), idvar = 'sbj', na.rm = T) %>%
  select(distractor, block, mean = First, se)
```

Explictly check the SEs:

```{r}
# rt SEs
soc_rt_se_1 <- rt_se %>% filter(distractor == 'social', block == 1) %>% pull(se)
soc_rt_se_2 <- rt_se %>% filter(distractor == 'social', block == 2) %>% pull(se)
soc_rt_se_3 <- rt_se %>% filter(distractor == 'social', block == 3) %>% pull(se)
nsoc_rt_se_1 <- rt_se %>% filter(distractor == 'nonsocial', block == 1) %>% pull(se)
nsoc_rt_se_2 <- rt_se %>% filter(distractor == 'nonsocial', block == 2) %>% pull(se)
nsoc_rt_se_3 <- rt_se %>% filter(distractor == 'nonsocial', block == 3) %>% pull(se)

reportobject <- compareValues2(reportedValue = "0.11", obtainedValue = soc_rt_se_1, valueType = 'se')
reportobject <- compareValues2(reportedValue = "0.06", obtainedValue = soc_rt_se_2, valueType = 'se')
reportobject <- compareValues2(reportedValue = "0.06", obtainedValue = soc_rt_se_3, valueType = 'se')
reportobject <- compareValues2(reportedValue = "0.11", obtainedValue = nsoc_rt_se_1, valueType = 'se')
reportobject <- compareValues2(reportedValue = "0.07", obtainedValue = nsoc_rt_se_2, valueType = 'se')
reportobject <- compareValues2(reportedValue = "0.07", obtainedValue = nsoc_rt_se_3, valueType = 'se')
```

```{r}
# accuracy SEs
soc_acc_se_1 <- acc_se %>% filter(distractor == 'social', block == 1) %>% pull(se)
soc_acc_se_2 <- acc_se %>% filter(distractor == 'social', block == 2) %>% pull(se)
soc_acc_se_3 <- acc_se %>% filter(distractor == 'social', block == 3) %>% pull(se)
nsoc_acc_se_1 <- acc_se %>% filter(distractor == 'nonsocial', block == 1) %>% pull(se)
nsoc_acc_se_2 <- acc_se %>% filter(distractor == 'nonsocial', block == 2) %>% pull(se)
nsoc_acc_se_3 <- acc_se %>% filter(distractor == 'nonsocial', block == 3) %>% pull(se)

reportobject <- compareValues2(reportedValue = "0.45", obtainedValue = soc_acc_se_1, valueType = 'se')
reportobject <- compareValues2(reportedValue = "0.34", obtainedValue = soc_acc_se_2, valueType = 'se')
reportobject <- compareValues2(reportedValue = "0.32", obtainedValue = soc_acc_se_3, valueType = 'se')
reportobject <- compareValues2(reportedValue = "0.64", obtainedValue = nsoc_acc_se_1, valueType = 'se')
reportobject <- compareValues2(reportedValue = "0.33", obtainedValue = nsoc_acc_se_2, valueType = 'se')
reportobject <- compareValues2(reportedValue = "0.34", obtainedValue = nsoc_acc_se_3, valueType = 'se')
```

![](images/table1.png)

```{r}
# first look SEs
soc_look_se_1 <- look_se %>% filter(distractor == 'social', block == 1) %>% pull(se)
soc_look_se_2 <- look_se %>% filter(distractor == 'social', block == 2) %>% pull(se)
soc_look_se_3 <- look_se %>% filter(distractor == 'social', block == 3) %>% pull(se)
nsoc_look_se_1 <- look_se %>% filter(distractor == 'nonsocial', block == 1) %>% pull(se)
nsoc_look_se_2 <- look_se %>% filter(distractor == 'nonsocial', block == 2) %>% pull(se)
nsoc_look_se_3 <- look_se %>% filter(distractor == 'nonsocial', block == 3) %>% pull(se)

reportobject <- compareValues2(reportedValue = "1.81", obtainedValue = soc_look_se_1, valueType = 'se')
reportobject <- compareValues2(reportedValue = "1.39", obtainedValue = soc_look_se_2, valueType = 'se')
reportobject <- compareValues2(reportedValue = "1.12", obtainedValue = soc_look_se_3, valueType = 'se')
reportobject <- compareValues2(reportedValue = "1.27", obtainedValue = nsoc_look_se_1, valueType = 'se')
reportobject <- compareValues2(reportedValue = "1.23", obtainedValue = nsoc_look_se_2, valueType = 'se')
reportobject <- compareValues2(reportedValue = "1.17", obtainedValue = nsoc_look_se_3, valueType = 'se')
```


Only minor errors remain here.

### Inferential statistics

We will now try to reproduce the following outcomes:

> A repeated-measures ANOVA with two within-subject factors (distractor: social, non-social; block: one, two, three), revealed a main effect of block, F(1.47, 52.89) = 395.23, p = < 0.001, η2 = 0.92, driven by decreasing search time across blocks. There was no main effect of distractor on search time, F(1, 36) = 0.38, p > 0.250, η2 = 0.01, but there was a significant distractor-by-block interaction, F(1.94, 69.98) = 3.78, p = 0.029, η2 = 0.10.

NOTE - we initially encounter an insufficient information error here as the non-integer degrees of freedom suggests that a correction had been used but was not specified. The authors clarified that they did use a Greenhouse-Geisser correction and also added that they used the ez.glm function (now known as the aov.ez function) from the afex package.

This was our first attempt to run a 2x3 within-subjects ANOVA with a greenhouse-geisser correction according to the authors instructions:

```{r}
# forAOV <- d.tidy %>% 
#   group_by(sbj, distractor, block) %>%
#   filter(accurate==1) %>%
#   dplyr::summarise(RT = log(mean(rt, na.rm = T)))
# 
# aov_out <- aov_ez(data = forAOV, id = "sbj", dv = "RT", within = c("distractor", "block"), correction = 'GG')
# pes <- anova(aov_out, es = 'pes')$pes # calc partial eta squared
# 
# kable(aov_out$anova_table, digits = 2)
# 
# # main effect of block, F(1.47, 52.89) = 395.23, p = < 0.001, η2 = 0.92
# compareValues(reportedValue = 52.89, obtainedValue = aov_out$anova_table$`den Df`[2]) # df2 ## minor numerical error
# compareValues(reportedValue = 395.23, obtainedValue = aov_out$anova_table$`F`[2]) # F ## minor numerical error
# compareValues(reportedValue = .92, obtainedValue = pes[2]) # pes
# # p value is reported as "p = < 0.001". That is a match.
# 
# # no main effect of distractor on search time, F(1, 36) = 0.38, p > 0.250, η2 = 0.01
# compareValues(reportedValue = 0.38, obtainedValue = aov_out$anova_table$`F`[1]) # F ## major numerical error
# compareValues(reportedValue = .01, obtainedValue = pes[1]) # pes
# # p value is reported as "p = < 0.250". That is a match.
# 
# # significant distractor-by-block interaction, F(1.94, 69.98) = 3.78, p = 0.029, η2 = 0.10
# compareValues(reportedValue = 1.95, obtainedValue = aov_out$anova_table$`num Df`[3]) # df1
# compareValues(reportedValue = 69.98, obtainedValue = aov_out$anova_table$`den Df`[3]) # df2 ## minor numerical error
# compareValues(reportedValue = 3.78, obtainedValue = aov_out$anova_table$`F`[3]) # F ## minor numerical error
# compareValues(reportedValue = .029, obtainedValue = aov_out$anova_table$`Pr(>F)`[3], isP = T) # p ## major numerical error
# compareValues(reportedValue = .10, obtainedValue = pes[3]) # pes
```

We encountered a number of errors (2 major) when we first ran this. The authors then informed us that some of the data was actually excluded and that this was not reported in the paper. They also provided some code to implement these exclusions and run the ANOVA (note we had to make some minor edits to get it to work):

```{r}
data <-read_csv('authorAssistance/Raw_data2.csv') # NB - TH edit, changed data.csv to read_csv

data2 <- data[(!is.nan(data$learn_total_search) & data$recognize ==0), ]

data3 <- data2[c("Sbj", "social", "block", "learn_total_search")]
data3 <- aggregate(data3$learn_total_search, by = list(Sbj=data3$Sbj, social=data3$social, block=data3$block), mean)
dataSUM <- summarySEwithin(data3, measurevar="x", withinvars=c("social","block"), idvar="Sbj", na.rm=FALSE, conf.interval=.95)
data3$x <- log(data3$x)


# model <- ez.glm("Sbj", "x", data3, within=c("social", "block"), factorize = FALSE, observed = c("x"), args.return = c("pes"))

aov_out <- aov_ez(data = data3, id = "Sbj", dv = "x", within = c("social", "block"), correction = 'GG') # NB - TH edit, updated to new aov function

pes <- anova(aov_out, es = 'pes')$pes # calc partial eta squared

kable(aov_out$anova_table, digits = 2)
```

Let's check these values:

```{r}
# main effect of block, F(1.47, 52.89) = 395.23, p = < 0.001, η2 = 0.92
reportObject <- compareValues2(reportedValue = "1.47", obtainedValue = aov_out$anova_table$`num Df`[2], valueType = 'df') # df1
reportObject <- compareValues2(reportedValue = "52.89", obtainedValue = aov_out$anova_table$`den Df`[2], valueType = 'df') # df2
reportObject <- compareValues2(reportedValue = "395.23", obtainedValue = aov_out$anova_table$`F`[2], valueType = 'F') # F
reportObject <- compareValues2(reportedValue = "eyeballMATCH", obtainedValue = aov_out$anova_table$`Pr(>F)`[2], valueType = 'p') # p value is reported as "p = < 0.001". That is a match.
reportObject <- compareValues2(reportedValue = ".92", obtainedValue = pes[2], valueType = 'es') # pes

# no main effect of distractor on search time, F(1, 36) = 0.38, p > 0.250, η2 = 0.01
reportObject <- compareValues2(reportedValue = "1", obtainedValue = aov_out$anova_table$`num Df`[1], valueType = 'df') # df1
reportObject <- compareValues2(reportedValue = "36", obtainedValue = aov_out$anova_table$`den Df`[1], valueType = 'df') # df2
reportObject <- compareValues2(reportedValue = "0.38", obtainedValue = aov_out$anova_table$`F`[1], valueType = 'F') # F
reportObject <- compareValues2(reportedValue = "eyeballMATCH", obtainedValue = aov_out$anova_table$`Pr(>F)`[1], valueType = 'p') # p value is reported as "p = < 0.250". That is a match. 
reportObject <- compareValues2(reportedValue = "0.01", obtainedValue = pes[1], valueType = 'es') # pes


# significant distractor-by-block interaction, F(1.94, 69.98) = 3.78, p = 0.029, η2 = 0.10
reportObject <- compareValues2(reportedValue = "1.94", obtainedValue = aov_out$anova_table$`num Df`[3], valueType = 'df') # df1
reportObject <- compareValues2(reportedValue = "69.98", obtainedValue = aov_out$anova_table$`den Df`[3], valueType = 'df') # df2
reportObject <- compareValues2(reportedValue = "3.78", obtainedValue = aov_out$anova_table$`F`[3], valueType = 'F') # F
reportObject <- compareValues2(reportedValue = "0.029", obtainedValue = aov_out$anova_table$`Pr(>F)`[3], valueType = 'p') # p
reportObject <- compareValues2(reportedValue = "0.10", obtainedValue = pes[3], valueType = 'es') # pes
```

Everything matches now (apart from one minor numerical error).

## Step 5: Conclusion

We were able to reproduce the means from Table 1. Initially, we could not reproduce the standard errors. The original authors informed us that they had used an R function to calculate within-subject standard errors using the Cousineau-Morey method. We were able to reproduce the values successfully after implementing this function. 

Initially, we did not attempt the ANOVA as it appeared that a correction had been applied and the correction was not identified. The authors told us that this was a Greenhouse-Geisser correction, and informed us of the R function they used. When we tried to implement the ANOVA using this R function, we still encountered two major numerical errors. The authors then informed us that some of the data had actually been excluded prior to running the main analysis - this was not reported in the paper. The authors also provided code to implement this exclusion and run the ANVOA. With some minor edits, we got this to work and were able to reproduce the target outcomes.

```{r}
reportObject$Article_ID <- "gIjYG"
reportObject$affectsConclusion <- NA
reportObject$error_typo <- 0
reportObject$error_specification <- 0
reportObject$error_analysis <- 0
reportObject$error_data <- 0
reportObject$error_unidentified <- 0
reportObject$Author_Assistance <- T
reportObject$resolved_typo <- 0
reportObject$resolved_specification <- 3
reportObject$resolved_analysis <- 0
reportObject$resolved_data <- 0
reportObject$correctionSuggested <- NA
reportObject$correctionPublished <- NA

# decide on final outcome
if(reportObject$Decision_Errors > 0 | reportObject$Major_Numerical_Errors > 0 | reportObject$Insufficient_Information_Errors > 0){
  reportObject$finalOutcome <- "Failure"
  if(reportObject$Author_Assistance == T){
    reportObject$finalOutcome <- "Failure despite author assistance"
  }
}else{
  reportObject$finalOutcome <- "Success"
  if(reportObject$Author_Assistance == T){
    reportObject$finalOutcome <- "Success with author assistance"
  }
}

# save the report object
filename <- paste0("reportObject_", reportObject$Article_ID,".csv")
write_csv(reportObject, filename)
```

## Report Object

```{r, echo = FALSE}
# display report object in chunks
kable(reportObject[2:10], align = 'l')
kable(reportObject[11:20], align = 'l')
kable(reportObject[21:25], align = 'l')
kable(reportObject[26:30], align = 'l')
kable(reportObject[31:35], align = 'l')
kable(reportObject[36:40], align = 'l')
kable(reportObject[41:45], align = 'l')
kable(reportObject[46:51], align = 'l')
kable(reportObject[52:57], align = 'l')
```

## Session information

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
