---
title: "COD Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

#### Article ID: gIjYG
#### Pilot: Minyoung Lee
#### Co-pilot: Tom Hardwicke
#### Start date: 03/24/2017
#### End date: 

-------

#### Methods summary: 

Participants searched for a target in complex scenes containing social or non-social distractors. The search time and whether or not the distractor was fixated first after scene onset (first look) were measured in each search trial. The search time and first look in social and non-social trials were compared over three blocks. Learning in visual search waas operationalized as decreasing search time over blocks.

------

#### Target outcomes: 

> 3.1. Visual search

> Manual and eye-tracking measures provided converging evidence for social stimuli being more distracting than non-social stimuli, with the effects interacting with learning over successive blocks (see Table 1 for descriptives).

> 3.1.1. Manual search time (s)

> A repeated-measures ANOVA with two within-subject factors (distractor: social, non-social; block: one, two, three), revealed a main effect of block, F(1.47, 52.89) = 395.23, p = < 0.001, η2 = 0.92, driven by decreasing search time across blocks. There was no main effect of distractor on search time, F(1, 36) = 0.38, p > 0.250, η2 = 0.01, but there was a significant distractor-by-block interaction, F(1.94, 69.98) = 3.78, p = 0.029, η2 = 0.10. Although Bonferroni-adjusted post hoc tests revealed no significant differences in search time between social and non-social scenes in each block separately (block one: p = 0.218, block two: p = 0.215, block three: p = 0.135), and highly significant differences between blocks for both social and non-social scenes separately (p < 0.001), there was a significant interaction between distractor and block in the linear contrasts, F(1, 36) = 5.13, p = 0.030, η2 = 0.13, suggesting a difference in slope across blocks between social and non-social scenes.

------

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

## Step 1: Load packages

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CODreports) # custom report functions
library(ez) # for anova
library(afex) # for anova (with author assistance)
library(plotrix) # for function to calculate standard error
```

## Step 2: Load data

```{r}
d <- read.csv("data/data.csv")
```

## Step 3: Tidy data

```{r}
d.tidy <- d %>% select(Sbj,social,block,learn_ACC,learn_total_search,Total_first_look,AQ,SAS) %>%
  rename(sbj = Sbj,
         accurate = learn_ACC,
         rt = learn_total_search,
         first = Total_first_look,
         distractor = social) %>%
  mutate(distractor = recode_factor(distractor, "1" = "social", "0" = "nonsocial"),
         sbj = factor(sbj),
         block = factor(block))
```

## Step 4: Run analysis

### Descriptive statistics

We will first try to reproduce the descriptive statistics reported in Table 1:

> Manual and eye-tracking measures provided converging evidence for social stimuli being more distracting than non-social stimuli, with the effects interacting with learning over successive blocks (see Table 1 for descriptives).

![](table1.png)


```{r}
# accuracy
tbl <- d.tidy %>% group_by(sbj, distractor, block) %>%
  summarise(accuracy = mean(accurate)*100)  %>%
  group_by(distractor, block) %>%
  summarise(meanAccuracy = mean(accuracy, na.rm = T), se_accuracy = std.error(accuracy, na.rm = T))

# RT and First look in accurate trials
tbl2 <- d.tidy %>% group_by(sbj, distractor, block) %>%
  filter(accurate==1) %>%
  summarise(RT = mean(rt), First = sum(first,na.rm=TRUE)/sum(!is.nan(first))*100) %>%
  group_by(distractor,block, add=FALSE) %>%
  summarise(meanRT = mean(RT), se_RT = std.error(RT), meanFirst = mean(First), se_first = std.error(First))

# join tables and make same format as Table 1
table1 <- left_join(tbl2, tbl) %>% select(distractor, block, meanRT, se_RT, meanAccuracy, se_accuracy, meanFirst, se_first)
kable(table1, digits = 2)
```

Visual comparison of our table 1 and the published table 1: all the means are accurately reproduced but all of the standard errors do no match.
       
Check discrepancies in standard errors with compare values function:

```{r}
# RT Standard Errors
compareValues(0.11, 0.17)
compareValues(0.06, 0.11)
compareValues(0.06, 0.08)
compareValues(0.11, 0.17)
compareValues(0.07, 0.14)
compareValues(0.07, 0.18)

# Accuracy Standard Errors
compareValues(0.45, 0.77)
compareValues(0.34, 0.59)
compareValues(0.32, 0.43)
compareValues(0.64, 0.78)
compareValues(0.33, 0.51)
compareValues(0.34, 0.65)

# First looks Standard Errors
compareValues(1.81, 2.44)
compareValues(1.39, 1.89)
compareValues(1.12, 1.49)
compareValues(1.27, 1.16)
compareValues(1.23, 1.09)
compareValues(1.17, 0.91)
```

NOTE: we asked the authors for assistance and they sent us more details. They said they used the following function to calculate within-subject standard errors:

```{r}
# define functions
# Gives count, mean, standard deviation, standard error of the mean, and confidence interval (default 95%).
#   data: a data frame.
#   measurevar: the name of a column that contains the variable to be summariezed
#   groupvars: a vector containing names of columns that contain grouping variables
#   na.rm: a boolean that indicates whether to ignore NA's
#   conf.interval: the percent range of the confidence interval (default is 95%)
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
                      conf.interval=.95, .drop=TRUE) {
    library(plyr)

    # New version of length which can handle NA's: if na.rm==T, don't count them
    length2 <- function (x, na.rm=FALSE) {
        if (na.rm) sum(!is.na(x))
        else       length(x)
    }

    # This does the summary. For each group's data frame, return a vector with
    # N, mean, and sd
    datac <- ddply(data, groupvars, .drop=.drop,
      .fun = function(xx, col) {
        c(N    = length2(xx[[col]], na.rm=na.rm),
          mean = mean   (xx[[col]], na.rm=na.rm),
          sd   = sd     (xx[[col]], na.rm=na.rm)
        )
      },
      measurevar
    )

    # Rename the "mean" column    
    datac <- rename(datac, c("mean" = measurevar))

    datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean

    # Confidence interval multiplier for standard error
    # Calculate t-statistic for confidence interval: 
    # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
    ciMult <- qt(conf.interval/2 + .5, datac$N-1)
    datac$ci <- datac$se * ciMult

    return(datac)
}

# Norms the data within specified groups in a data frame; it normalizes each
# subject (identified by idvar) so that they have the same mean, within each group
# specified by betweenvars.
#   data: a data frame.
#   idvar: the name of a column that identifies each subject (or matched subjects)
#   measurevar: the name of a column that contains the variable to be summariezed
#   betweenvars: a vector containing names of columns that are between-subjects variables
#   na.rm: a boolean that indicates whether to ignore NA's
normDataWithin <- function(data=NULL, idvar, measurevar, betweenvars=NULL,
                           na.rm=FALSE, .drop=TRUE) {
    library(plyr)

    # Measure var on left, idvar + between vars on right of formula.
    data.subjMean <- ddply(data, c(idvar, betweenvars), .drop=.drop,
     .fun = function(xx, col, na.rm) {
        c(subjMean = mean(xx[,col], na.rm=na.rm))
      },
      measurevar,
      na.rm
    )

    # Put the subject means with original data
    data <- merge(data, data.subjMean)

    # Get the normalized data in a new column
    measureNormedVar <- paste(measurevar, "_norm", sep="")
    data[,measureNormedVar] <- data[,measurevar] - data[,"subjMean"] +
                               mean(data[,measurevar], na.rm=na.rm)

    # Remove this subject mean column
    data$subjMean <- NULL

    return(data)
}

# Summarizes data, handling within-subjects variables by removing inter-subject variability.
# It will still work if there are no within-S variables.
# Gives count, un-normed mean, normed mean (with same between-group mean),
#   standard deviation, standard error of the mean, and confidence interval.
# If there are within-subject variables, calculate adjusted values using method from Morey (2008).
#   data: a data frame.
#   measurevar: the name of a column that contains the variable to be summariezed
#   betweenvars: a vector containing names of columns that are between-subjects variables
#   withinvars: a vector containing names of columns that are within-subjects variables
#   idvar: the name of a column that identifies each subject (or matched subjects)
#   na.rm: a boolean that indicates whether to ignore NA's
#   conf.interval: the percent range of the confidence interval (default is 95%)
summarySEwithin <- function(data=NULL, measurevar, betweenvars=NULL, withinvars=NULL,
                            idvar=NULL, na.rm=FALSE, conf.interval=.95, .drop=TRUE) {

  # Ensure that the betweenvars and withinvars are factors
  factorvars <- vapply(data[, c(betweenvars, withinvars), drop=FALSE],
    FUN=is.factor, FUN.VALUE=logical(1))

  if (!all(factorvars)) {
    nonfactorvars <- names(factorvars)[!factorvars]
    message("Automatically converting the following non-factors to factors: ",
            paste(nonfactorvars, collapse = ", "))
    data[nonfactorvars] <- lapply(data[nonfactorvars], factor)
  }

  # Get the means from the un-normed data
  datac <- summarySE(data, measurevar, groupvars=c(betweenvars, withinvars),
                     na.rm=na.rm, conf.interval=conf.interval, .drop=.drop)

  # Drop all the unused columns (these will be calculated with normed data)
  datac$sd <- NULL
  datac$se <- NULL
  datac$ci <- NULL

  # Norm each subject's data
  ndata <- normDataWithin(data, idvar, measurevar, betweenvars, na.rm, .drop=.drop)

  # This is the name of the new column
  measurevar_n <- paste(measurevar, "_norm", sep="")

  # Collapse the normed data - now we can treat between and within vars the same
  ndatac <- summarySE(ndata, measurevar_n, groupvars=c(betweenvars, withinvars),
                      na.rm=na.rm, conf.interval=conf.interval, .drop=.drop)

  # Apply correction from Morey (2008) to the standard error and confidence interval
  #  Get the product of the number of conditions of within-S variables
  nWithinGroups    <- prod(vapply(ndatac[,withinvars, drop=FALSE], FUN=nlevels,
                           FUN.VALUE=numeric(1)))
  correctionFactor <- sqrt( nWithinGroups / (nWithinGroups-1) )

  # Apply the correction factor
  ndatac$sd <- ndatac$sd * correctionFactor
  ndatac$se <- ndatac$se * correctionFactor
  ndatac$ci <- ndatac$ci * correctionFactor

  # Combine the un-normed means with the normed results
  merge(datac, ndatac)
}
```

Apply the function:

```{r}
d.tidy %>% 
  group_by(sbj, distractor, block) %>%
  dplyr::summarise(accuracy = mean(accurate)*100) %>% # aggregate first
  summarySEwithin(measurevar = 'accuracy', withinvars = c('distractor', 'block'), idvar = 'sbj', na.rm = T) %>%
  select(distractor, block, mean = accuracy, se) %>%
  kable(digits = 2)
```

![](table1.png)
MATCH.

Now check RT for accurate trials only:

```{r}
d.tidy %>% 
  group_by(sbj, distractor, block) %>%
  filter(accurate==1) %>%
  dplyr::summarise(RT = mean(rt), First = sum(first,na.rm=TRUE)/sum(!is.nan(first))*100) %>% # first aggregate
  summarySEwithin(measurevar = 'RT', withinvars = c('distractor', 'block'), idvar = 'sbj', na.rm = T) %>%
  select(distractor, block, mean = RT, se) %>%
  kable(digits = 2)
```
MATCH.

Now check first look for accurate trials only:

```{r}
d.tidy %>% 
  group_by(sbj, distractor, block) %>%
  filter(accurate==1) %>%
  dplyr::summarise(First = mean(first, na.rm = T)*100) %>% # first aggregate
  summarySEwithin(measurevar = 'First', withinvars = c('distractor', 'block'), idvar = 'sbj', na.rm = T) %>%
  select(distractor, block, mean = First, se) %>%
  kable(digits = 2)
```

![](table1.png)

Means MATCH. The standard errors for first looks are still slightly off, let's check the size of the discrepancies:

```{r}
compareValues(reportedValue = 1.81, obtainedValue = 1.86)
compareValues(reportedValue = 1.39, obtainedValue = 1.43)
compareValues(reportedValue = 1.12, obtainedValue = 1.15)

compareValues(reportedValue = 1.27, obtainedValue = 1.30)
compareValues(reportedValue = 1.23, obtainedValue = 1.26)
compareValues(reportedValue = 1.17, obtainedValue = 1.20)
```

Only minor errors remain here.

### Inferential statistics

We will now try to reproduce the following outcomes:

> A repeated-measures ANOVA with two within-subject factors (distractor: social, non-social; block: one, two, three), revealed a main effect of block, F(1.47, 52.89) = 395.23, p = < 0.001, η2 = 0.92, driven by decreasing search time across blocks. There was no main effect of distractor on search time, F(1, 36) = 0.38, p > 0.250, η2 = 0.01, but there was a significant distractor-by-block interaction, F(1.94, 69.98) = 3.78, p = 0.029, η2 = 0.10.

INSUFFICIENT INFORMATION ERROR - non-integer degrees of freedom suggests that a correction has been used, perhaps Greenhouse-Geisser?

NOTE - the authors clarified that they did use a Greenhouse-Geisser correction and said they used the ez.glm function (now known as the aov.ez function) from the afex package.

Run a 2x3 within-subjects ANOVA:

```{r}
forAOV <- d.tidy %>% 
  group_by(sbj, distractor, block) %>%
  filter(accurate==1) %>%
  dplyr::summarise(RT = log(mean(rt, na.rm = T)))

aov_out <- aov_ez(data = forAOV, id = "sbj", dv = "RT", within = c("distractor", "block"), correction = 'GG')
pes <- anova(aov_out, es = 'pes')$pes # calc partial eta squared

kable(aov_out$anova_table, digits = 2)

# main effect of block, F(1.47, 52.89) = 395.23, p = < 0.001, η2 = 0.92
compareValues(reportedValue = 52.89, obtainedValue = aov_out$anova_table$`den Df`[2]) # df2
compareValues(reportedValue = 395.23, obtainedValue = aov_out$anova_table$`F`[2]) # F
compareValues(reportedValue = .92, obtainedValue = pes[2]) # pes
# p value is reported as "p = < 0.001". That is a match.

# no main effect of distractor on search time, F(1, 36) = 0.38, p > 0.250, η2 = 0.01
compareValues(reportedValue = 0.38, obtainedValue = aov_out$anova_table$`F`[1]) # F
compareValues(reportedValue = .01, obtainedValue = pes[1]) # pes
# p value is reported as "p = < 0.250". That is a match.

# significant distractor-by-block interaction, F(1.94, 69.98) = 3.78, p = 0.029, η2 = 0.10
compareValues(reportedValue = 1.95, obtainedValue = aov_out$anova_table$`num Df`[3]) # df1
compareValues(reportedValue = 69.98, obtainedValue = aov_out$anova_table$`den Df`[3]) # df2
compareValues(reportedValue = 3.78, obtainedValue = aov_out$anova_table$`F`[3]) # F
compareValues(reportedValue = .029, obtainedValue = aov_out$anova_table$`Pr(>F)`[3], isP = T) # p
compareValues(reportedValue = .10, obtainedValue = pes[3]) # pes
```

Due to two major numerical errors in the main anova, we will not proceed with the post-hoc tests.


Note - the authors provided some code for running the ANOVA:
```{r}
data <-read_csv('data/data.csv')

data2 <- data[!is.nan(data$learn_total_search), ]

data3 <- data2[c("Sbj", "social", "block", "learn_total_search")]
data3 <- aggregate(data3$learn_total_search, by = list(Sbj=data3$Sbj, social=data3$social, block=data3$block), mean)
data3$x <- log(data3$x)

# below does not run - throws an error: Observed variable not in data: x
#model <- ez.glm("Sbj", "x", data3, within=c("social", "block"),  factorize = FALSE, 
                 #observed = c("x"), args.return = c("pes"))
       
# run the above but with the observed argument (whcih apparently is only needed for calculating generalised eta squared which is not relevant here)          
model <- ez.glm("Sbj", "x", data3, within=c("social", "block"),  factorize = FALSE, args.return = c("pes"))
kable(model$anova_table, digits = 2)
```

This matches the output from the anova we calculated above.

## Step 5: Conclusion

```{r}
codReport(Report_Type = 'joint',
          Article_ID = 'gIjYG',
          Insufficient_Information_Errors = 0,
          Decision_Errors = 0,
          Major_Numerical_Errors = 2,
          Minor_Numerical_Errors = 10,
          Author_Assistance = T)
```

We were able to reproduce the means from Table 1. Initially, we could not reproduce the standard errors, however after the original authors informed us that they had used an R function to calculat within-subject standard errors, and we were able to reproduce the values successfully after implementing this function. 

Initially, we did not attempt the ANOVA as it appeared that a correction had been applied and the correction was not identified. The authors told us that this was a Greenhouse-Geisser correction, and informed us of the R function they used. When we tried to implement the ANOVA using this R function, we still encountered two major numerical errors.


```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
